{"cells":[{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":2520,"status":"ok","timestamp":1719392437751,"user":{"displayName":"Nigel Lim","userId":"01538512243568498299"},"user_tz":-480},"id":"2h8sFpVJoC9n"},"outputs":[],"source":["import os\n","import pickle\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","import joblib\n","import pandas as pd\n","import numpy as np\n","\n","current_working_directory = os.getcwd()\n","\n","class Model:\n","  def __init__(self, current_working_directory):\n","    # list required arguments and their types\n","    self.__list_args = ['postcode', 'year', 'month', 'property_landed']\n","    self.__list_argtypes = ['list of str', 'int', 'int', 'list of int']\n","\n","    # get the current working directory and corresponding filenames\n","    self.__dict_filename = current_working_directory + '/data/year_postcode_big_dict.pickle'\n","    self.__colnames_filename = current_working_directory + '/data/colnames.pickle'\n","    self.__pred_model_filename = current_working_directory + '/data/pred_model.pickle'\n","    self.__spi_full_data_filename = current_working_directory + '/data/spi_full_data.csv'\n","    self.__defactorized_price_unit_log_scaler_filename = current_working_directory + '/data/defactorized_price_unit_log.gz'\n","    self.__lat_scaler_filename = current_working_directory + '/data/lat.gz'\n","    self.__long_scaler_filename = current_working_directory + '/data/long.gz'\n","    self.__mrt_500m_exist_scaler_filename = current_working_directory + '/data/mrt_500m_exist.gz'\n","    self.__mrt_1km_exist_scaler_filename = current_working_directory + '/data/mrt_1km_exist.gz'\n","    self.__mrt_500m_1y_scaler_filename = current_working_directory + '/data/mrt_500m_1y.gz'\n","    self.__mrt_1km_1y_scaler_filename = current_working_directory + '/data/mrt_1km_1y.gz'\n","    self.__mrt_500m_3y_scaler_filename = current_working_directory + '/data/mrt_500m_3y.gz'\n","    self.__mrt_1km_3y_scaler_filename = current_working_directory + '/data/mrt_1km_3y.gz'\n","    self.__mrt_500m_5y_scaler_filename = current_working_directory + '/data/mrt_500m_5y.gz'\n","    self.__mrt_1km_5y_scaler_filename = current_working_directory + '/data/mrt_1km_5y.gz'\n","    self.__schools_500m_scaler_filename = current_working_directory + '/data/schools_500m.gz'\n","    self.__schools_1km_scaler_filename = current_working_directory + '/data/schools_1km.gz'\n","    self.__malls_500m_scaler_filename = current_working_directory + '/data/malls_500m.gz'\n","    self.__malls_1km_scaler_filename = current_working_directory + '/data/malls_1km.gz'\n","    self.__onehotencoder_filename = current_working_directory + '/data/onehotencoder.gz'\n","\n","    # get year-postcode big dictionary\n","    with open(self.__dict_filename, 'rb') as handle:\n","      self.__year_postcode_big_dict = pickle.load(handle)\n","\n","    self.__lines = (\"\"\"\n","        01 ,01, 02, 03, 04, 05, 06\n","        02 ,07, 08\n","        03, 14, 15, 16\n","        04 ,09, 10\n","        05 ,11, 12, 13\n","        06 ,17\n","        07 ,18, 19\n","        08, 20, 21\n","        09, 22, 23\n","        10, 24, 25, 26, 27\n","        11 ,28, 29, 30\n","        12 ,31, 32, 33\n","        13 ,34, 35, 36, 37\n","        14 ,38, 39, 40, 41\n","        15 ,42, 43, 44, 45\n","        16 ,46, 47, 48\n","        17 ,49, 50, 81\n","        18 ,51, 52\n","        19 ,53, 54, 55, 82\n","        20 ,56, 57\n","        21 ,58, 59\n","        22 ,60, 61, 62, 63, 64\n","        23 ,65, 66, 67, 68\n","        24 ,69, 70, 71\n","        25 ,72, 73\n","        26 ,77, 78\n","        27 ,75, 76\n","        28 ,79, 80\n","        \"\"\").strip().split('\\n')\n","    self.__mapping_dict = {}\n","    for line in self.__lines:\n","      parts = line.split(',')\n","      first_value = int(parts[0].strip())  # Convert the first value to integer\n","      for value in parts[1:]:\n","        self.__mapping_dict[int(value.strip())] = first_value  # Convert the keys to integers\n","\n","    # get list of column names in the order they appear in the model's training df\n","    with open(self.__colnames_filename, 'rb') as file:\n","      self.__colnames = pickle.load(file)\n","    pred_var = \"defactorized_price_unit_log_norm\"\n","    self.__colnames.remove(pred_var)\n","\n","    # get predictive model\n","    with open(self.__pred_model_filename, 'rb') as file:\n","      self.__pred_model = pickle.load(file)\n","\n","    # get spi full data\n","    self.__spi_full_data = pd.read_csv(self.__spi_full_data_filename)\n","\n","    # get scalers\n","    self.__defactorized_price_unit_log_scaler = joblib.load(self.__defactorized_price_unit_log_scaler_filename)\n","    self.__lat_scaler = joblib.load(self.__lat_scaler_filename)\n","    self.__long_scaler = joblib.load(self.__long_scaler_filename)\n","    self.__mrt_500m_exist_scaler = joblib.load(self.__mrt_500m_exist_scaler_filename)\n","    self.__mrt_1km_exist_scaler = joblib.load(self.__mrt_1km_exist_scaler_filename)\n","    self.__mrt_500m_1y_scaler = joblib.load(self.__mrt_500m_1y_scaler_filename)\n","    self.__mrt_1km_1y_scaler = joblib.load(self.__mrt_1km_1y_scaler_filename)\n","    self.__mrt_500m_3y_scaler = joblib.load(self.__mrt_500m_3y_scaler_filename)\n","    self.__mrt_1km_3y_scaler = joblib.load(self.__mrt_1km_3y_scaler_filename)\n","    self.__mrt_500m_5y_scaler = joblib.load(self.__mrt_500m_5y_scaler_filename)\n","    self.__mrt_1km_5y_scaler = joblib.load(self.__mrt_1km_5y_scaler_filename)\n","    self.__schools_500m_scaler = joblib.load(self.__schools_500m_scaler_filename)\n","    self.__schools_1km_scaler = joblib.load(self.__schools_1km_scaler_filename)\n","    self.__malls_500m_scaler = joblib.load(self.__malls_500m_scaler_filename)\n","    self.__malls_1km_scaler = joblib.load(self.__malls_1km_scaler_filename)\n","\n","    # get one-hot encoder\n","    self.__onehotencoder = joblib.load(self.__onehotencoder_filename)\n","\n","  def __get_info(self, postcodes, year, col_name, year_postcode_big_dict):\n","    ### gets information (latitude, longitude, postal district, market segment, facility no) for each postcode and a given year from\n","    ### pre-computed dictionary containing info by given year and postcode\n","    postcode_dict = self.__year_postcode_big_dict.get(year)\n","    val_dicts = list(map(postcode_dict.get, postcodes))\n","    vals = list(map(lambda x: x.get(col_name), val_dicts))\n","    return vals\n","\n","  def __get_district_from_postal(self, postal_code):\n","    # gets a postal code as integer or string\n","    # outputs the postal district (int)\n","    first_two_digits = int(str(postal_code)[:-4])\n","    return self.__mapping_dict.get(first_two_digits, \"Unknown\")\n","\n","  def __get_post_district(self, postcodes):\n","    # gets a list of postcodes\n","    # outputs a list of postal districts (int)\n","    post_district_list = list(map(lambda x: self.__get_district_from_postal(x), postcodes))\n","    return post_district_list\n","\n","  def __get_segment_from_district(self, post_district):\n","    if post_district in [9, 10, 11]:\n","      return 'Core Central Region'\n","    elif post_district < 9 or post_district in [12, 13, 14, 15, 20]:\n","      return 'Rest of Central Region'\n","    else:\n","      return 'Outside Central Region'\n","\n","  def __get_market_segment(self, post_district_list):\n","    # gets a list of integers of postal districts\n","    # outputs a list of market segments\n","    market_segment_list = list(map(lambda x: self.__get_segment_from_district(x), post_district_list))\n","    return market_segment_list\n","\n","  def __scale_var(self, var, var_norm, df, scaler):\n","    column = df[var].values.reshape(-1, 1)\n","    normalized_column = scaler.transform(column)\n","    df = df.drop(var, axis=1)\n","    return df.assign(**{var_norm: normalized_column})\n","\n","  def __unscale_var(self, var_norm, var, df, scaler):\n","    column = df[var_norm].values.reshape(-1, 1)\n","    unnormalized_column = scaler.inverse_transform(column)\n","    df = df.drop(var_norm, axis=1)\n","    return df.assign(**{var: unnormalized_column})\n","\n","  def __dummify_vars(self, vars, df, encoder):\n","    df_num = df[df.columns.difference(vars)]\n","    df_cat = df[vars]\n","    df_encoded_array = encoder.transform(df_cat)\n","    categorical_columns = [f'{col}_{cat}' for i, col in enumerate(df_cat.columns) for cat in encoder.categories_[i]]\n","    df_encoded = pd.DataFrame(df_encoded_array, columns = categorical_columns)\n","    df = df_num.join(df_encoded)\n","    return df\n","\n","  def __exp_var(self, var, var_exp, df):\n","    column = df[var].values.reshape(-1, 1)\n","    exp_column = np.exp(column)\n","    df = df.drop(var, axis=1)\n","    return df.assign(**{var_exp: exp_column})\n","\n","  def __multiply_spi(self, price_var, refactorized_price_var, spi_var, df):\n","    price_column = df[price_var].values.reshape(-1, 1)\n","    spi_column = df[spi_var].values.reshape(-1, 1)\n","    refactorized_price_column = np.multiply(price_column, spi_column)\n","    df = df.drop(price_var, axis=1)\n","    df = df.drop(spi_var, axis=1)\n","    return df.assign(**{refactorized_price_var: refactorized_price_column})\n","\n","  def __time_to_units(self, df, ref_year):\n","    df['time'] = (df['year'] - ref_year) + (df['month'] * 1/12)\n","    df = df.drop(columns=['year', 'month'])\n","    return df\n","\n","  def __check_num_args(self, args, num_args):\n","    try:\n","      if len(args) != num_args:\n","        raise ValueError()\n","    except ValueError:\n","      print('Please input ' + str(num_args) + ' arguments (use required_args method to list the arguments needed).')\n","\n","  def __check_all_types_correct(self, objs, obj_types, descriptors, obj_type_strings):\n","    try:\n","      objs_with_wrong_types = []\n","      objs_with_wrong_type_strings = []\n","      for obj, obj_type, descriptor, obj_type_string in zip(objs, obj_types, descriptors, obj_type_strings):\n","        if (type(obj_type) is list):\n","          if not (type(obj) in obj_type):\n","            objs_with_wrong_types.append(descriptor)\n","            objs_with_wrong_type_strings.append(obj_type_string)\n","        elif not (type(obj) is obj_type):\n","          objs_with_wrong_types.append(descriptor)\n","          objs_with_wrong_type_strings.append(obj_type_string)\n","      if len(objs_with_wrong_types) > 0:\n","        raise ValueError()\n","    except ValueError:\n","      print('The following objects are of wrong types: ' + ', '.join(objs_with_wrong_types) + '.')\n","      print('They should be of respective types: ' + ', '.join(objs_with_wrong_type_strings) + '.')\n","\n","  def __check_all_lengths_equal(self, lsts):\n","    try:\n","      it = iter(lsts)\n","      the_len = len(next(it))\n","      if not all(len(l) == the_len for l in it):\n","        raise ValueError()\n","    except ValueError:\n","      print('Not all lists have same length.')\n","\n","  def __check_all_lst_types_correct(self, lsts, lst_types, lst_names, lst_type_strings):\n","    try:\n","      lsts_with_wrong_types = []\n","      lsts_with_wrong_type_strings = []\n","      for lst, lst_type, lst_name, lst_type_string in zip(lsts, lst_types, lst_names, lst_type_strings):\n","        if (type(lst_type) is list):\n","          if not all(type(elt) in lst_type for elt in lst):\n","            lsts_with_wrong_types.append(lst_name)\n","            lsts_with_wrong_type_strings.append(lst_type_string)\n","        elif not all(type(elt) is lst_type for elt in lst):\n","          lsts_with_wrong_types.append(lst_name)\n","          lsts_with_wrong_type_strings.append(lst_type_string)\n","      if len(lsts_with_wrong_types) > 0:\n","        raise ValueError()\n","    except ValueError:\n","      print('The following lists have some elements with wrong types: ' + ', '.join(lsts_with_wrong_types) + '.')\n","      print('They should contain elements of respective types: ' + ', '.join(lsts_with_wrong_type_strings) + '.')\n","\n","  def __check_year_correct(self, year, valid_years, valid_years_descriptor):\n","    try:\n","      if year not in valid_years:\n","        raise ValueError()\n","    except ValueError:\n","      print('Please input a year in the range ' + valid_years_descriptor + '.')\n","\n","  def __check_postcodes_exist(self, postcodes, postcode_dict):\n","    try:\n","      postcodes_notin_markers = map(lambda x: x not in postcode_dict, postcodes)\n","      nonexistent_postcodes = []\n","      for postcode, notin_marker in zip(postcodes, postcodes_notin_markers):\n","        if notin_marker:\n","          nonexistent_postcodes.append(postcode)\n","      if nonexistent_postcodes:\n","        raise ValueError()\n","    except ValueError:\n","      print('The following postcodes are invalid: ' + ', '.join(nonexistent_postcodes) + '.')\n","\n","  def __check_month_correct(self, month, valid_months, valid_months_descriptor):\n","    try:\n","      if month not in valid_months:\n","        raise ValueError()\n","    except ValueError:\n","      print('Please input a month in the range ' + valid_months_descriptor + '.')\n","\n","  def __check_members(self, lst, valid_members, variable_descriptor):\n","    try:\n","      invalid_member_markers = map(lambda x: x not in valid_members, lst)\n","      invalid_members = []\n","      for member, invalid_member_marker in zip(lst, invalid_member_markers):\n","        if invalid_member_marker:\n","          invalid_members.append(member)\n","      if invalid_members:\n","        raise ValueError()\n","    except ValueError:\n","      print('The following ' + variable_descriptor + ' are invalid: ' + ', '.join(invalid_members) + '.')\n","      print('Please input ' + variable_descriptor + ' from the following: ' + ', '.join(valid_members) + '.')\n","\n","  def __get_predictions(self, postcodes, year, month, property_landed):\n","    lat = self.__get_info(postcodes, year, \"latitude\", self.__year_postcode_big_dict)\n","    long = self.__get_info(postcodes, year, \"longitude\", self.__year_postcode_big_dict)\n","    malls_1km = self.__get_info(postcodes, year, \"malls_1km\", self.__year_postcode_big_dict)\n","    malls_500m = self.__get_info(postcodes, year, \"malls_500m\", self.__year_postcode_big_dict)\n","    mrt_1km_1y = self.__get_info(postcodes, year, \"mrt_1km_1y\", self.__year_postcode_big_dict)\n","    mrt_1km_3y = self.__get_info(postcodes, year, \"mrt_1km_3y\", self.__year_postcode_big_dict)\n","    mrt_1km_5y = self.__get_info(postcodes, year, \"mrt_1km_5y\", self.__year_postcode_big_dict)\n","    mrt_1km_exist = self.__get_info(postcodes, year, \"mrt_1km_exist\", self.__year_postcode_big_dict)\n","    mrt_500m_1y = self.__get_info(postcodes, year, \"mrt_500m_1y\", self.__year_postcode_big_dict)\n","    mrt_500m_3y = self.__get_info(postcodes, year, \"mrt_500m_3y\", self.__year_postcode_big_dict)\n","    mrt_500m_5y = self.__get_info(postcodes, year, \"mrt_500m_5y\", self.__year_postcode_big_dict)\n","    mrt_500m_exist = self.__get_info(postcodes, year, \"mrt_500m_exist\", self.__year_postcode_big_dict)\n","    schools_1km = self.__get_info(postcodes, year, \"schools_1km\", self.__year_postcode_big_dict)\n","    schools_500m = self.__get_info(postcodes, year, \"schools_500m\", self.__year_postcode_big_dict)\n","    postal_district = self.__get_post_district(postcodes)\n","    market_segment = self.__get_market_segment(postal_district)\n","    length = len(postcodes)\n","    year = [year] * length\n","    month = [month] * length\n","    data = {\"lat\": lat,\n","            \"long\": long,\n","            \"malls_1km\": malls_1km,\n","            \"malls_500m\": malls_500m,\n","            \"mrt_1km_1y\": mrt_1km_1y,\n","            \"mrt_1km_3y\": mrt_1km_3y,\n","            \"mrt_1km_5y\": mrt_1km_5y,\n","            \"mrt_1km_exist\": mrt_1km_exist,\n","            \"mrt_500m_1y\": mrt_500m_1y,\n","            \"mrt_500m_3y\": mrt_500m_3y,\n","            \"mrt_500m_5y\": mrt_500m_5y,\n","            \"mrt_500m_exist\": mrt_500m_exist,\n","            \"schools_1km\": schools_1km,\n","            \"schools_500m\": schools_500m,\n","            \"postal_district\": postal_district,\n","            \"property_landed\": property_landed,\n","            \"market_segment\": market_segment,\n","            \"year\": year,\n","            \"month\": month}\n","    df = pd.DataFrame(data)\n","    df = self.__time_to_units(df, 2019)\n","    df = self.__scale_var('lat', 'lat_norm', df, self.__lat_scaler)\n","    df = self.__scale_var('long', 'long_norm', df, self.__long_scaler)\n","    df = self.__scale_var('mrt_500m_exist', 'mrt_500m_exist_norm', df, self.__mrt_500m_exist_scaler)\n","    df = self.__scale_var('mrt_1km_exist', 'mrt_1km_exist_norm', df, self.__mrt_1km_exist_scaler)\n","    df = self.__scale_var('mrt_500m_1y', 'mrt_500m_1y_norm', df, self.__mrt_500m_1y_scaler)\n","    df = self.__scale_var('mrt_1km_1y', 'mrt_1km_1y_norm', df, self.__mrt_1km_1y_scaler)\n","    df = self.__scale_var('mrt_500m_3y', 'mrt_500m_3y_norm', df, self.__mrt_500m_3y_scaler)\n","    df = self.__scale_var('mrt_1km_3y', 'mrt_1km_3y_norm', df, self.__mrt_1km_3y_scaler)\n","    df = self.__scale_var('mrt_500m_5y', 'mrt_500m_5y_norm', df, self.__mrt_500m_5y_scaler)\n","    df = self.__scale_var('mrt_1km_5y', 'mrt_1km_5y_norm', df, self.__mrt_1km_5y_scaler)\n","    df = self.__scale_var('schools_500m', 'schools_500m_norm', df, self.__schools_500m_scaler)\n","    df = self.__scale_var('schools_1km', 'schools_1km_norm', df, self.__schools_1km_scaler)\n","    df = self.__scale_var('malls_500m', 'malls_500m_norm', df, self.__malls_500m_scaler)\n","    df = self.__scale_var('malls_1km', 'malls_1km_norm', df, self.__malls_1km_scaler)\n","    cat_vars = ['postal_district', 'market_segment']\n","    df = self.__dummify_vars(cat_vars, df, self.__onehotencoder)\n","    df = df[self.__colnames]\n","    defactorized_price_unit_log_norm = self.__pred_model.predict(df)\n","    df.insert(0, \"postcode\", postcodes)\n","    df = df.assign(defactorized_price_unit_log_norm = defactorized_price_unit_log_norm)\n","    # on defactorized_price_unit_log_norm, do unscale_var -> np.exp -> multiply by appropriate spi values to get prices\n","    df = self.__unscale_var(\"defactorized_price_unit_log_norm\", \"defactorized_price_unit_log\", df, self.__defactorized_price_unit_log_scaler)\n","    df = self.__exp_var(\"defactorized_price_unit_log\", \"defactorized_price_unit\", df)\n","    df = df.assign(market_segment = market_segment)\n","    df = df.assign(year = year)\n","    df = df.assign(month = month)\n","    df = pd.merge(df, self.__spi_full_data, how = \"left\", on = ['year', 'month', 'property_landed', 'market_segment'])\n","    df = self.__multiply_spi(\"defactorized_price_unit\", \"price_unit\", \"spi\", df)\n","    return df\n","\n","  def __get_predictions_lean(self, postcodes, year, month, property_landed):\n","    final_colnames = self.__list_args + [\"price_unit\"]\n","    df = self.__get_predictions(postcodes, year, month, property_landed)\n","    df = df[final_colnames]\n","    return df\n","\n","  def predict(self, *args):\n","    # run checks for valid inputs\n","    req_objs = [\"postcode\", \"year\", \"month\", \"property_landed\"]\n","    num_args = len(req_objs)\n","    self.__check_num_args(args, num_args)\n","\n","    postcodes, year, month, property_landed = args\n","    objs = [postcodes, year, month, property_landed]\n","    obj_types = [list, int, int, list]\n","    descriptors = [\"postcode\", \"year\", \"month\", \"property_landed\"]\n","    obj_type_strings = ['list', 'int', 'int', 'list']\n","    self.__check_all_types_correct(objs, obj_types, descriptors, obj_type_strings)\n","\n","    lsts = [postcodes, property_landed]\n","    self.__check_all_lengths_equal(lsts)\n","    lst_types = [str, int]\n","    lst_names = [\"postcode\", \"property_landed\"]\n","    lst_type_strings = ['str', 'int']\n","    self.__check_all_lst_types_correct(lsts, lst_types, lst_names, lst_type_strings)\n","\n","    valid_years = list(range(2019, 2051))\n","    self.__check_year_correct(year, valid_years, '2019 to 2050')\n","\n","    postcode_dict = self.__year_postcode_big_dict.get(year)\n","    self.__check_postcodes_exist(postcodes, postcode_dict)\n","\n","    valid_months = list(range(1, 13))\n","    valid_months_descriptor = '1 to 12'\n","    self.__check_month_correct(month, valid_months, valid_months_descriptor)\n","\n","    # add check for property_landed\n","\n","    return self.__get_predictions_lean(postcodes, year, month, property_landed)\n","\n","  def required_args(self):\n","    print('The required arguments are: ' + ', '.join(list(map(lambda x: x[0] + ' (' + x[1] + ')', zip(self.__list_args, self.__list_argtypes)))) + '.')"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1719392444868,"user":{"displayName":"Nigel Lim","userId":"01538512243568498299"},"user_tz":-480},"id":"wlOHvjlghGLr","outputId":"108079d6-f665-4d19-85fe-105eae91a0a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["To use the model, create an instance of Model with the path of the current working directory (a string) as argument, where the required model data files are in a subdirectory named data. Then use the predict method in this Model instance to produce predictions (in a DataFrame) for the given inputs. Use the required_args method to print the arguments required for the predict method.\n"]}],"source":["print(\"To use the model, create an instance of Model with the path of the current working directory (a string) as argument, where the required model data files are in a subdirectory named data. Then use the predict method in this Model instance to produce predictions (in a DataFrame) for the given inputs. Use the required_args method to print the arguments required for the predict method.\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOhi6kXRyErb3db8ATnonTJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
